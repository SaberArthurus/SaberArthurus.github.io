---
title: 'Survey on Query-Based Adversarial Attack'
date: 2020-03-07
permalink: /posts/2020/03/Survey-Query-Based/
tags:
  - Survey
  - Adversarial Attack
---

# Summary

Adversarial attack refers to perturb input in a limited range to misclassify the machine learning algorithm. Averagely speaking, when the attackers acquire every detail of the machine learning algorithm, the machine learning models are easily attacked. However, in the real scenario, the design details of machine learning algorithm is almost confidential to the attackers except from the result of models. Hence, the research community are in an attempt to estimate an adversarial perturbation for a specific input whereby prediction result. The adversarial noise estimation often involves a considerable number of queries therefore we call this kind of attack as query-based attack. In the next section, we will introduce several significant query-based attack works to sort out the query-based attack clues.


## [Query-Efficient Black-box Adversarial Examples](https://arxiv.org/abs/1712.07113v2)
This is an earlier query-based attack algorithm which describes the application of Natural Evolutionary Strategies on adversarial attack. The main process is described as the following: First, sample the noise variant sharing the same dimension with the input  according to Gaussian Distribution. Second, employ antithetic sampling to generate a batch of Gaussian noises. Third, calculate the expected value of the inner product between the Gaussian output and Model output.
Noticeably, the adversarial noise can be utilized in constructed model settings the output of which can be either hard label or top-k output.


Query-limited Black-box Attacks to Classifiers (https://arxiv.org/pdf/1712.08713.pdf)
This paper aims at decreasing the number of queries. Specifically, the query would return only one class and corresponding confidence. The author decrease the query number to tenth through a Bayesian Optimization compared to random strategy. Meanwhile, the modification cost is low as well. Authors mentioned DIRECT algorithm as their adopted optimization strategy.


Query-Efficient Hard-Label Black-box Attack: An Optimization-based Approach (https://arxiv.org/pdf/1807.04457.pdf)
This paper proposed a novel way to solve zeroth order optimization algorithm. Instead of random walk on the boundary, the authors make the full use of the property of boundary by a binary search algorithm. The experiments fully demonstrate that the binary search approach outperforms previous random walk algorithm. Particularly, proposed algorithm can attack other discrete machine learning algorithm such as GDBT.

Towards Query Efficient Black-box Attacks: An Input-free Perspective (https://arxiv.org/pdf/1809.02918.pdf)
This creative point of this paper is that the adversarial perturbation generation does not require a given input. This paper is just to start with a gray image and utilize NES algorithm to perturb the gray image. Because the gray image situates in the uncertainty area, it is intractable to estimate the global gradient direction by NES strategy. This paper adopted region mutation way, namely, only computing the gradient in a local manifold tiling to the entire image. Through this way, the approach achieve the objective of input-free.
Query-Efficient Black-Box Attack by Active Learning(https://arxiv.org/pdf/1809.04913.pdf)

Adversarial examples is to attack neural network by adding small perturbations to the raw input. This paper focus on transfer-based black attack which refers to craft adversarial examples through a substitute model. Apparently, there is a gap between substitute model and victim model. Hence, this paper leverages this issue by constructing informative examples to fine-tune substitute model. The construction process is as followed: (1) generate adversarial examples (2) find most informative adv examples by querying the victim model prediction (3) use informative adversarial examples to retrain substitute model.

Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks (https://arxiv.org/pdf/1906.04392.pdf)
Previous decision-based black-box attack based on the random vector query without any prior information. However, transfer-based adversarial attacks have a promising result in adversarial attack. Hence, the motivation of this paper is to span these promising subspaces vectors generated by a few reference models. To prevent the generated prior noises from falling into local minima, the authors approached it by bringing in drop-out/layer strategy. Their method can gain up to 2x and 4x reductions in the requisite mean and medium numbers of queries even if the reference models are trained on a less relevant dataset.

Universal Decision-Based Black-Box Perturbations: Breaking Security-Through-Obscurity Defenses(https://arxiv.org/pdf/1811.03733.pdf)
This paper proves that the existence of a universal noise vector that adversaries can estimate under a hard-label (decision-based)  black-box setting. Their proposed approach is to combine binary search and universal perturbation generation. 

A geometry-inspired decision-based attack(https://arxiv.org/pdf/1903.10826.pdf)
The core idea of geometry is to estimate the gradients at the boundary area instead of input points. This paper assumed that the gradient is in the flatten area and therefore the gradients at the boundary area can be applied in the input images. This approaches is conducive to batch-process the input and acquire the gradients accurately. This paper also defines a low-dimensional subspace calculated by DCT to achieve a higher efficiency.

Query-efficient Meta Attacker to Deep Neural Networks (https://arxiv.org/pdf/1906.02398.pdf)
This paper leverage the decision-based black-box  through train an auto-encoder to estimate the input gradient. The proposed method is to utilize the meta learning strategy to provide a reasonable initialization for auto-encoder.  Apart from it, to fine-tune the auto-encoder for the neural network, this algorithm queries many coordinate maps to retrain the auto-encoder. To minimize the time cost, this paper performs the retraining operation at a regular interval.

RED-Attack: Resource Efficient Decision-Based Imperceptible Attack for Machine Learning(https://arxiv.org/pdf/1901.10258.pdf)
This paper mitigated the black-box decision-based attack by estimating the boundary of neural networks. First, they address the boundary estimation by binary search method. Second, perturb the boundary image,  estimate the gradient, and take the combination of gradient and boundary image as next query input. Third, repeat the above steps and shrink the step size into 1/2. The shrinkage of step size contributes to converge better and find an adversary with a minimum distance.


Black-Box Decision based Adversarial Attack with Symmetric a-stable distribution(https://arxiv.org/pdf/1904.05586.pdf)
With incremental innovation, this paper utilizes the a-stable distribution instead of Gaussian distribution to perform random walk search.


Efficient Decision-based Black-box Adversarial Attacks on Face Recognition (https://arxiv.org/pdf/1904.04433.pdf)
This paper introduced the Covariance Matrix Adaption which can model the local geometries of the search directions. In fact, better gradient estimation means better efficiency. Therefore, the intuitive exploration direction is to take full use of the past successful directions. So the covariance matrix adaption update will enlarge the useful directions to future search. The way to apply covariance matrix adaption in the decision-based adversarial attack is to sample coordinate vector according a matrix. During search process, the matrix is updating continuously. However, the computation complexity of this matrix is huge O(N^2), and to reduce the computation cost this paper only focuses on dig 
.
